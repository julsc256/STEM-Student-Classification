{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**\n",
    "- Due to sensitive material being gathered, some information has been altered to keep privacy of students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/juliettec/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('df4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with one comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = nltk.sent_tokenize(df1['Comments'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No Comment .': 1, \"'Demonstrates above average ability, motivation and attitude in this class.\": 1, \"'Meets acceptable standards of cooperation, work, attitude and effort .\": 5, \"'Conscientious about classroom responsibilities.\": 1}\n"
     ]
    }
   ],
   "source": [
    "def count_vectorize(comment, vocab=None):\n",
    "    if vocab:\n",
    "        unique_words = vocab\n",
    "    else:\n",
    "        unique_words = list(set(comment))\n",
    "    \n",
    "    comment_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in comment:\n",
    "        comment_dict[word] += 1\n",
    "    \n",
    "    return comment_dict\n",
    "\n",
    "test_vectorized = count_vectorize(testing)\n",
    "print(test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments=[]\n",
    "for i in df1['Comments']:\n",
    "    tokenized_comments.append(nltk.sent_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_count = []\n",
    "for i in tokenized_comments:\n",
    "    comment_count.append(count_vectorize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\"'Meets acceptable standards of cooperation, work, attitude and effort .\": 3,\n",
       "  'No Comment .': 1,\n",
       "  \"'Demonstrates above average ability, motivation and attitude in this class.\": 2,\n",
       "  \"'Demonstrates outstanding effort.\": 1,\n",
       "  \"'Conscientious about classroom responsibilities.\": 1},\n",
       " {'No Comment .': 1,\n",
       "  \"'Demonstrates above average ability, motivation and attitude in this class.\": 1,\n",
       "  \"'Meets acceptable standards of cooperation, work, attitude and effort .\": 5,\n",
       "  \"'Conscientious about classroom responsibilities.\": 1},\n",
       " {\"'Poor test scores.\": 1,\n",
       "  \"'Incomplete work.\": 3,\n",
       "  'No Comment .': 2,\n",
       "  \"'Meets acceptable standards of cooperation, work, attitude and effort .\": 2},\n",
       " {\"'Meets acceptable standards of cooperation, work, attitude and effort .\": 1,\n",
       "  'No Comment .': 2,\n",
       "  \"'Demonstrates above average ability, motivation and attitude in this class.\": 3,\n",
       "  \"'Demonstrates outstanding effort.\": 1,\n",
       "  \"'Conscientious about classroom responsibilities.\": 1},\n",
       " {'No Comment .': 1,\n",
       "  \"'Incomplete work.\": 1,\n",
       "  \"'Meets acceptable standards of cooperation, work, attitude and effort .\": 4,\n",
       "  \"'Demonstrates outstanding effort.\": 2},\n",
       " {'No Comment .': 1,\n",
       "  \"'Demonstrates above average ability, motivation and attitude in this class.\": 2,\n",
       "  \"'Meets acceptable standards of cooperation, work, attitude and effort .\": 3,\n",
       "  \"'Need put more effort for Chinese'.\": 2},\n",
       " {\"'Meets acceptable standards of cooperation, work, attitude and effort .\": 2,\n",
       "  \"'Late work = points off .\": 1,\n",
       "  'No Comment .': 1,\n",
       "  \"'Demonstrates above average ability, motivation and attitude in this class.\": 1,\n",
       "  \"'Unprepared for class.\": 2,\n",
       "  \"'Conscientious about classroom responsibilities.\": 1},\n",
       " {\"'Meets acceptable standards of cooperation, work, attitude and effort .\": 2,\n",
       "  'No Comment .': 2,\n",
       "  \"Satisfactory'.\": 1,\n",
       "  \"'Good work.\": 1,\n",
       "  \"'Demonstrates outstanding effort.\": 2,\n",
       "  \"'Conscientious about classroom responsibilities.\": 1},\n",
       " {'No Comment .': 2,\n",
       "  \"'Inattentiveness in class.\": 2,\n",
       "  \"'Meets acceptable standards of cooperation, work, attitude and effort .\": 3,\n",
       "  \"'Conscientious about classroom responsibilities.\": 1},\n",
       " {'No Comment .': 2,\n",
       "  \"'Incomplete homework.\": 1,\n",
       "  \"'Meets acceptable standards of cooperation, work, attitude and effort .\": 4,\n",
       "  \"'Poor test scores.\": 1}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This gives us all the unique words that are found in the dataframe\n",
    "    - downside is that if new comments are loaded, this would not compensate for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'about', 'above', 'absences', 'absent', 'academic', 'acceptable', 'according', 'accumulated', 'add', 'advantage', 'affecting', 'after', 'all', 'along', 'also', 'always', 'am', 'an', 'and', 'appropriate', 'area', 'areas', 'as', 'ask', 'asking', 'assigned', 'assignments', 'at', 'attendance', 'attention', 'attentive', 'attentiveness', 'attitude', 'austin', 'available', 'average', 'away', 'be', 'because', 'been', 'before', 'behavior', 'benefitting', 'better', 'big', 'bit', 'both', 'bright', 'brought', 'but', 'came', 'can', 'catch', 'cell', 'chatting', 'check', 'chinese', 'class', 'classroom', 'classwork', 'come', 'comes', 'commendable', 'comment', 'complete', 'completed', 'completely', 'conference', 'conferences', 'conscientious', 'content', 'continues', 'cooperation', 'cooperatively', 'could', 'counselor', 'currently', 'daily', 'danger', 'date', 'day', 'deal', 'definitely', 'delight', 'demonstrates', 'developed', 'did', 'directions', 'discussed', 'disruptive', 'do', 'does', 'doesn', 'doing', 'don', 'done', 'door', 'down', 'due', 'during', 'earn', 'easily', 'effort', 'eighty', 'email', 'end', 'enjoy', 'entered', 'entrant', 'entry', 'especially', 'ever', 'every', 'excessive', 'exercises', 'extra', 'extremely', 'failing', 'family', 'fifty', 'focus', 'follow', 'for', 'four', 'from', 'games', 'get', 'gets', 'giving', 'go', 'goals', 'going', 'good', 'goofing', 'google', 'grade', 'grades', 'great', 'habit', 'habits', 'had', 'handed', 'hard', 'harder', 'has', 'have', 'having', 'he', 'help', 'her', 'hi', 'him', 'his', 'home', 'homework', 'hope', 'however', 'hundred', 'if', 'improve', 'improvement', 'improvements', 'improvisation', 'in', 'inattentiveness', 'including', 'incomplete', 'insightful', 'instruction', 'instructions', 'into', 'is', 'issue', 'it', 'joy', 'just', 'keep', 'keeping', 'kid', 'kind', 'large', 'late', 'lately', 'lateness', 'learning', 'lesson', 'listen', 'lot', 'low', 'made', 'make', 'management', 'many', 'marking', 'me', 'meaningfully', 'meeting', 'meets', 'missed', 'missing', 'moment', 'month', 'more', 'most', 'motivation', 'mp', 'much', 'multiple', 'must', 'my', 'need', 'needed', 'needs', 'never', 'new', 'next', 'no', 'not', 'noted', 'notes', 'number', 'of', 'off', 'offered', 'on', 'one', 'only', 'opened', 'opportunities', 'or', 'other', 'our', 'out', 'outstanding', 'over', 'overall', 'owes', 'parent', 'part', 'participate', 'participating', 'participation', 'pass', 'peers', 'per', 'percent', 'performance', 'period', 'phone', 'piece', 'playing', 'please', 'pleased', 'point', 'points', 'poor', 'practice', 'project', 'projects', 'pronunciation', 'put', 'puts', 'questions', 'quieter', 'quite', 'quiz', 'reach', 'recall', 'received', 'recent', 'recently', 'reflective', 'requested', 'requirements', 'responsibilities', 'retake', 'review', 'right', 'risk', 'rushes', 'satisfactory', 'school', 'scores', 'second', 'see', 'seen', 'semester', 'sessions', 'seven', 'she', 'should', 'show', 'showed', 'showing', 'shown', 'shows', 'so', 'some', 'specific', 'standards', 'started', 'stay', 'still', 'stop', 'student', 'studied', 'sure', 'take', 'talk', 'talking', 'teach', 'teacher', 'test', 'tests', 'than', 'that', 'the', 'them', 'thirty', 'this', 'those', 'through', 'time', 'timeline', 'times', 'to', 'told', 'too', 'towards', 'transferred', 'tried', 'tries', 'trying', 'tutoring', 'twice', 'two', 'understand', 'unprepared', 'until', 'up', 'us', 'value', 'very', 'video', 'was', 'we', 'weakness', 'well', 'what', 'when', 'which', 'while', 'will', 'with', 'wonderful', 'work', 'working', 'worth', 'writing', 'yet', 'you', 'zero']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df1['Comments'])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This gives us the number of times the words show up (column) per student (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = X.toarray() #number of words per student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 2, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This gives us the total number of words per row, using numpy.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_num_words = np.array([np.sum(i) for i in word_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Needed to find the average word use:\n",
    "    - `word_count.T` because the shape was not the same, had to transpose it to be the same\n",
    "    - `(word_count.T/total_num_words).T` the whole thing, in order to have it in the shape for all students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the mean number of words\n",
    "# do something to reduce the dimensions (Top 100 words used) - they are in abc order look \n",
    "# get_features\n",
    "# downfall of it - words that won't be used later\n",
    "average_word_use = (word_count.T/total_num_words).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word_count = pd.DataFrame(average_word_use, columns=[vectorizer.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>absences</th>\n",
       "      <th>absent</th>\n",
       "      <th>academic</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>according</th>\n",
       "      <th>accumulated</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>worth</th>\n",
       "      <th>writing</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows Ã— 363 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ability     about     above absences absent academic acceptable  \\\n",
       "0    0.035714  0.017857  0.035714      0.0    0.0      0.0   0.053571   \n",
       "1    0.016393  0.016393  0.016393      0.0    0.0      0.0   0.081967   \n",
       "2    0.000000  0.000000  0.000000      0.0    0.0      0.0   0.064516   \n",
       "3    0.060000  0.020000  0.060000      0.0    0.0      0.0   0.020000   \n",
       "4    0.000000  0.000000  0.000000      0.0    0.0      0.0   0.086957   \n",
       "..        ...       ...       ...      ...    ...      ...        ...   \n",
       "881  0.000000  0.000000  0.000000      0.0    0.0      0.0   0.000000   \n",
       "882  0.048387  0.000000  0.048387      0.0    0.0      0.0   0.048387   \n",
       "883  0.000000  0.000000  0.000000      0.0    0.0      0.0   0.025000   \n",
       "884  0.000000  0.000000  0.000000      0.0    0.0      0.0   0.051282   \n",
       "885  0.000000  0.000000  0.000000      0.0    0.0      0.0   0.000000   \n",
       "\n",
       "    according accumulated  add  ... will with wonderful      work working  \\\n",
       "0         0.0         0.0  0.0  ...  0.0  0.0       0.0  0.053571     0.0   \n",
       "1         0.0         0.0  0.0  ...  0.0  0.0       0.0  0.081967     0.0   \n",
       "2         0.0         0.0  0.0  ...  0.0  0.0       0.0  0.161290     0.0   \n",
       "3         0.0         0.0  0.0  ...  0.0  0.0       0.0  0.020000     0.0   \n",
       "4         0.0         0.0  0.0  ...  0.0  0.0       0.0  0.108696     0.0   \n",
       "..        ...         ...  ...  ...  ...  ...       ...       ...     ...   \n",
       "881       0.0         0.0  0.0  ...  0.0  0.0       0.0  0.000000     0.0   \n",
       "882       0.0         0.0  0.0  ...  0.0  0.0       0.0  0.048387     0.0   \n",
       "883       0.0         0.0  0.0  ...  0.0  0.0       0.0  0.050000     0.0   \n",
       "884       0.0         0.0  0.0  ...  0.0  0.0       0.0  0.051282     0.0   \n",
       "885       0.0         0.0  0.0  ...  0.0  0.0       0.0  0.000000     0.0   \n",
       "\n",
       "    worth writing  yet  you zero  \n",
       "0     0.0     0.0  0.0  0.0  0.0  \n",
       "1     0.0     0.0  0.0  0.0  0.0  \n",
       "2     0.0     0.0  0.0  0.0  0.0  \n",
       "3     0.0     0.0  0.0  0.0  0.0  \n",
       "4     0.0     0.0  0.0  0.0  0.0  \n",
       "..    ...     ...  ...  ...  ...  \n",
       "881   0.0     0.0  0.0  0.0  0.0  \n",
       "882   0.0     0.0  0.0  0.0  0.0  \n",
       "883   0.0     0.0  0.0  0.0  0.0  \n",
       "884   0.0     0.0  0.0  0.0  0.0  \n",
       "885   0.0     0.0  0.0  0.0  0.0  \n",
       "\n",
       "[886 rows x 363 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New df with new word count averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.concat([df1, average_word_count], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_csv('df5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to add as features:\n",
    "- sentiment analysis - load pass comments through - probability make that a feature\n",
    "- find the percentage of the num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
